<!DOCTYPE html>
<html>

  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Stefan Grünewald | publications</title>
<meta name="description" content="Personal website of Stefan Grünewald
">

<!-- JQuery -->
<script src="/assets/js/jquery-3.5.1.min.js"></script>

<!-- Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/manni.css">
 


<!-- Bootstrap & MDB -->
<link href="/assets/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<script src="/assets/js/bootstrap.min.js"></script>

<!-- Fonts & Icons -->
<link rel="stylesheet" href="/assets/css/fontawesome/css/all.min.css">
<link rel="stylesheet" href="/assets/css/academicons/css/academicons.css">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Open Graph -->


  </head>

  <body class="sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top">
    <div class="container">
      
      
      
      
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Stefan</span>   Grünewald
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/cv/">
                cv
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">My publications in reverse chronological order.</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2023</h2>
  <ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge wiesp"><a href="https://aclanthology.org/venues/wiesp/" target="_blank" rel="external nofollow noopener">WIESP</a></abbr>
    
  
  </div>

  <div id="schrader-etal-2023-mulms-multi" class="col-sm-8">
    
      <span class="title">MuLMS: A Multi-Layer Annotated Text Corpus for Information Extraction in the Materials Science Domain</span>
      <span class="author">
        
          
            
              
                  Timo Pierre Schrader,
              
            
          
        
          
            
              
                  Matteo Finco,
              
            
          
        
          
            
              
                  Stefan Grünewald,
              
            
          
        
          
            
              
                  Felix Hildebrand,
              
            
          
        
          
            
              
                  and <a href="https://annefried.github.io/" target="_blank" rel="external nofollow noopener">Annemarie Friedrich</a>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In: Proceedings of the Second Workshop on Information Extraction from Scientific Publications</em>
      
      <!--
        2023
      -->
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abstract</a>]
    
    
    
    
      [<a href="/assets/pdf/wiesp2023_mulms.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Keeping track of all relevant recent publications and experimental results for a research area is a challenging task. Prior work has demonstrated the efficacy of information extraction models in various scientific areas. Recently, several datasets have been released for the yet understudied materials science domain. However, these datasets focus on sub-problems such as parsing synthesis procedures or on subdomains, e.g., solid oxide fuel cells. In this resource paper, we present MuLMS, a new dataset of 50 open-access articles, spanning seven sub-domains of materials science. The corpus has been annotated by domain experts with several layers ranging from named entities over relations to frame structures. We present competitive neural models for all tasks and demonstrate that multi-task training with existing related resources leads to benefits.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge findings"><a href="https://aclanthology.org/venues/findings/" target="_blank" rel="external nofollow noopener">Findings</a></abbr>
    
  
  </div>

  <div id="henning-etal-2022-mist" class="col-sm-8">
    
      <span class="title">MiST: a Large-Scale Annotated Resource and Neural Models for Functions of Modal Verbs in English Scientific Text</span>
      <span class="author">
        
          
            
              
                  Sophie Henning,
              
            
          
        
          
            
              
                  Nicole Macher,
              
            
          
        
          
            
              
                  Stefan Grünewald,
              
            
          
        
          
            
              
                  and <a href="https://annefried.github.io/" target="_blank" rel="external nofollow noopener">Annemarie Friedrich</a>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In: Findings of the Association for Computational Linguistics: EMNLP 2022</em>
      
      <!--
        2022
      -->
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abstract</a>]
    
    
    
    
      [<a href="/assets/pdf/emnlp2022_mist.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Modal verbs (e.g., can, should or must) occur highly frequently in scientific articles. Decoding their function is not straightforward: they are often used for hedging, but they may also denote abilities and restrictions. Understanding their meaning is important for accurate information extraction from scientific text. To foster research on the usage of modals in this genre, we introduce the MIST (Modals In Scientific Text) dataset, which contains 3737 modal instances in five scientific domains annotated for their semantic, pragmatic, or rhetorical function. We systematically evaluate a set of competitive neural architectures on MIST. Transfer experiments reveal that leveraging non-scientific data is of limited benefit for modeling the distinctions in MIST. Our corpus analysis provides evidence that scientific communities differ in their usage of modal verbs, yet, classifiers trained on scientific data generalize to some extent to unseen scientific domains.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge law"><a href="https://aclanthology.org/venues/law/" target="_blank" rel="external nofollow noopener">LAW</a></abbr>
    
  
  </div>

  <div id="burkle-etal-2021-corpus" class="col-sm-8">
    
      <span class="title">A Corpus Study of Creating Rule-Based Enhanced Universal Dependencies for German</span>
      <span class="author">
        
          
            
              
                  Teresa Bürkle,
              
            
          
        
          
            
              
                  Stefan Grünewald,
              
            
          
        
          
            
              
                  and <a href="https://annefried.github.io/" target="_blank" rel="external nofollow noopener">Annemarie Friedrich</a>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In: Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop</em>
      
      <!--
        2021
      -->
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abstract</a>]
    
    
    
    
      [<a href="/assets/pdf/law2021_german_ud.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this paper, we present a first attempt at enriching German Universal Dependencies (UD) treebanks with enhanced dependencies. Similarly to the converter for English (Schuster and Manning, 2016), we develop a rule-based system for deriving enhanced dependencies from the basic layer, covering three linguistic phenomena: relative clauses, coordination, and raising/control. For quality control, we manually correct or validate a set of 196 sentences, finding that around 90% of added relations are correct. Our data analysis reveals that difficulties arise mainly due to inconsistencies in the basic layer annotations. We show that the English system is in general applicable to German data, but that adapting to the particularities of the German treebanks and language increases precision and recall by up to 10%. Comparing the application of our converter on gold standard dependencies vs. automatic parses, we find that F1 drops by around 10% in the latter setting due to error propagation. Finally, an enhanced UD parser trained on a converted treebank performs poorly when evaluated against our annotations, indicating that more work remains to be done to create gold standard enhanced German treebanks.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge conll"><a href="https://aclanthology.org/venues/conll/" target="_blank" rel="external nofollow noopener">CoNLL</a></abbr>
    
  
  </div>

  <div id="sineva-etal-2021-negation" class="col-sm-8">
    
      <span class="title">Negation-Instance Based Evaluation of End-to-End Negation Resolution</span>
      <span class="author">
        
          
            
              
                  Elizaveta Sineva,
              
            
          
        
          
            
              
                  Stefan Grünewald,
              
            
          
        
          
            
              
                  <a href="https://annefried.github.io/" target="_blank" rel="external nofollow noopener">Annemarie Friedrich</a>,
              
            
          
        
          
            
              
                  and <a href="https://www.ims.uni-stuttgart.de/en/institute/team/Kuhn-00013/" target="_blank" rel="external nofollow noopener">Jonas Kuhn</a>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In: Proceedings of the 25th Conference on Computational Natural Language Learning</em>
      
      <!--
        2021
      -->
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abstract</a>]
    
    
    
    
      [<a href="/assets/pdf/conll2021_negation.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this paper, we revisit the task of negation resolution, which includes the subtasks of cue detection (e.g. “not”, “never”) and scope resolution. In the context of previous shared tasks, a variety of evaluation metrics have been proposed. Subsequent works usually use different subsets of these, including variations and custom implementations, rendering meaningful comparisons between systems difficult. Examining the problem both from a linguistic perspective and from a downstream viewpoint, we here argue for a negation-instance based approach to evaluating negation resolution. Our proposed metrics correspond to expectations over per-instance scores and hence are intuitively interpretable. To render research comparable and to foster future work, we provide results for a set of current state-of-the-art systems for negation resolution on three English corpora, and make our implementation of the evaluation scripts publicly available.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge iwpt"><a href="https://aclanthology.org/venues/iwpt/" target="_blank" rel="external nofollow noopener">IWPT</a></abbr>
    
  
  </div>

  <div id="grunewald-etal-2021-robertnlp" class="col-sm-8">
    
      <span class="title">RobertNLP at the IWPT 2021 Shared Task: Simple Enhanced UD Parsing for 17 Languages</span>
      <span class="author">
        
          
            
              
                  Stefan Grünewald,
              
            
          
        
          
            
              
                  Frederik Tobias Oertel,
              
            
          
        
          
            
              
                  and <a href="https://annefried.github.io/" target="_blank" rel="external nofollow noopener">Annemarie Friedrich</a>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In: Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021)</em>
      
      <!--
        2021
      -->
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abstract</a>]
    
    
    
    
      [<a href="/assets/pdf/iwpt2021_sharedtask.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper presents our multilingual dependency parsing system as used in the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies. Our system consists of an unfactorized biaffine classifier that operates directly on fine-tuned XLM-R embeddings and generates enhanced UD graphs by predicting the best dependency label (or absence of a dependency) for each pair of tokens. To avoid sparsity issues resulting from lexicalized dependency labels, we replace lexical items in relations with placeholders at training and prediction time, later retrieving them from the parse via a hybrid rule-based/machine-learning system. In addition, we utilize model ensembling at prediction time. Our system achieves high parsing accuracy on the blind test data, ranking 3rd out of 9 with an average ELAS F1 score of 86.97.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge iwpt"><a href="https://aclanthology.org/venues/iwpt/" target="_blank" rel="external nofollow noopener">IWPT</a></abbr>
    
  
  </div>

  <div id="grunewald-etal-2021-applying" class="col-sm-8">
    
      <span class="title">Applying Occam’s Razor to Transformer-Based Dependency Parsing: What Works, What Doesn’t, and What is Really Necessary</span>
      <span class="author">
        
          
            
              
                  Stefan Grünewald,
              
            
          
        
          
            
              
                  <a href="https://annefried.github.io/" target="_blank" rel="external nofollow noopener">Annemarie Friedrich</a>,
              
            
          
        
          
            
              
                  and <a href="https://www.ims.uni-stuttgart.de/en/institute/team/Kuhn-00013/" target="_blank" rel="external nofollow noopener">Jonas Kuhn</a>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In: Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021)</em>
      
      <!--
        2021
      -->
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abstract</a>]
    
    
    
    
      [<a href="/assets/pdf/iwpt2021_occam.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>The introduction of pre-trained transformer-based contextualized word embeddings has led to considerable improvements in the accuracy of graph-based parsers for frameworks such as Universal Dependencies (UD). However, previous works differ in various dimensions, including their choice of pre-trained language models and whether they use LSTM layers. With the aims of disentangling the effects of these choices and identifying a simple yet widely applicable architecture, we introduce STEPS, a new modular graph-based dependency parser. Using STEPS, we perform a series of analyses on the UD corpora of a diverse set of languages. We find that the choice of pre-trained embeddings has by far the greatest impact on parser performance and identify XLM-R as a robust choice across the languages in our study. Adding LSTM layers provides no benefits when using transformer-based embeddings. A multi-task training setup outputting additional UD features may contort results. Taking these insights together, we propose a simple but widely applicable parser architecture and configuration, achieving new state-of-the-art results (in terms of LAS) for 10 out of 12 diverse languages.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge arxiv"><a href="https://arxiv.org/" target="_blank" rel="external nofollow noopener">arXiv</a></abbr>
    
  
  </div>

  <div id="grunewald-2021-mst" class="col-sm-8">
    
      <span class="title">Maximum Spanning Trees Are Invariant to Temperature Scaling in Graph-based Dependency Parsing</span>
      <span class="author">
        
          
            Stefan Grünewald
          
        
      </span>

      <span class="periodical">
      
      <!--
        2021
      -->
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abstract</a>]
    
    
    
    
      [<a href="/assets/pdf/mst_temp_proof.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Modern graph-based syntactic dependency parsers operate by predicting, for each token within a sentence, a probability distribution over its possible syntactic heads (i.e., all other tokens) and then extracting a maximum spanning tree from the resulting log-probabilities. Nowadays, virtually all such parsers utilize deep neural networks and may thus be susceptible to miscalibration (in particular, overconfident predictions). In this paper, we prove that temperature scaling, a popular technique for post-hoc calibration of neural networks, cannot change the output of the aforementioned procedure. We conclude that other techniques are needed to tackle miscalibration in graph-based dependency parsers in a way that improves parsing accuracy. </p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge eacl"><a href="https://aclanthology.org/venues/eacl/" target="_blank" rel="external nofollow noopener">EACL</a></abbr>
    
  
  </div>

  <div id="grunewald-etal-2021-coordinate" class="col-sm-8">
    
      <span class="title">Coordinate Constructions in English Enhanced Universal Dependencies: Analysis and Computational Modeling</span>
      <span class="author">
        
          
            
              
                  Stefan Grünewald,
              
            
          
        
          
            
              
                  Prisca Piccirilli,
              
            
          
        
          
            
              
                  and <a href="https://annefried.github.io/" target="_blank" rel="external nofollow noopener">Annemarie Friedrich</a>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In: Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>
      
      <!--
        2021
      -->
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abstract</a>]
    
    
    
    
      [<a href="/assets/pdf/coord_paper.pdf" target="_blank">PDF</a>]
    
    
    
      [<a href="/assets/pdf/coord_poster.pdf" target="_blank">Poster</a>]
    
    
      [<a href="/assets/pdf/coord_slides.pdf" target="_blank">Slides</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this paper, we address the representation of coordinate constructions in Enhanced Universal Dependencies (UD), where relevant dependency links are propagated from conjunction heads to other conjuncts. English treebanks for enhanced UD have been created from gold basic dependencies using a heuristic rule-based converter, which propagates only core arguments. With the aim of determining which set of links should be propagated from a semantic perspective, we create a large-scale dataset of manually edited syntax graphs. We identify several systematic errors in the original data, and propose to also propagate adjuncts. We observe high inter-annotator agreement for this semantic annotation task. Using our new manually verified dataset, we perform the first principled comparison of rule-based and (partially novel) machine-learning based methods for conjunction propagation for English. We show that learning propagation rules is more effective than hand-designing heuristic rules. When using automatic parses, our neural graph-parser based edge predictor outperforms the currently predominant pipelines using a basic-layer tree parser plus converters.</p>
    </span>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge udw"><a href="https://aclanthology.org/venues/udw/" target="_blank" rel="external nofollow noopener">UDW</a></abbr>
    
  
  </div>

  <div id="grunewald-friedrich-2020-unifying" class="col-sm-8">
    
      <span class="title">Unifying the Treatment of Preposition-Determiner Contractions in German Universal Dependencies Treebanks</span>
      <span class="author">
        
          
            
              
                  Stefan Grünewald
              
            
          
        
          
            
              
                  and <a href="https://annefried.github.io/" target="_blank" rel="external nofollow noopener">Annemarie Friedrich</a>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In: Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020)</em>
      
      <!--
        2020
      -->
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abstract</a>]
    
    
    
    
      [<a href="/assets/pdf/contractions_paper.pdf" target="_blank">PDF</a>]
    
    
    
      [<a href="/assets/pdf/contractions_poster.pdf" target="_blank">Poster</a>]
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>HDT-UD, the largest German UD treebank by a large margin, as well as the German-LIT treebank, currently do not analyze preposition-determiner contractions such as zum (= zu dem, “to the“) as multi-word tokens, which is inconsistent both with UD guidelines as well as other German UD corpora (GSD and PUD). In this paper, we show that harmonizing corpora with regard to this highly frequent phenomenon using a lookup-table based approach leads to a considerable increase in automatic parsing performance.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge iwpt"><a href="https://aclanthology.org/venues/iwpt/" target="_blank" rel="external nofollow noopener">IWPT</a></abbr>
    
  
  </div>

  <div id="grunewald-friedrich-2020-robertnlp" class="col-sm-8">
    
      <span class="title">RobertNLP at the IWPT 2020 Shared Task: Surprisingly Simple Enhanced UD Parsing for English</span>
      <span class="author">
        
          
            
              
                  Stefan Grünewald
              
            
          
        
          
            
              
                  and <a href="https://annefried.github.io/" target="_blank" rel="external nofollow noopener">Annemarie Friedrich</a>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In: Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies</em>
      
      <!--
        2020
      -->
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abstract</a>]
    
    
    
    
      [<a href="/assets/pdf/robertnlp_paper.pdf" target="_blank">PDF</a>]
    
    
    
    
      [<a href="/assets/pdf/robertnlp_slides.pdf" target="_blank">Slides</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper presents our system at the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies. Using a biaffine classifier architecture (Dozat and Manning, 2017) which operates directly on finetuned RoBERTa embeddings, our parser generates enhanced UD graphs by predicting the best dependency label (or absence of a dependency) for each pair of tokens in the sentence. We address label sparsity issues by replacing lexical items in relations with placeholders at prediction time, later retrieving them from the parse in a rule-based fashion. In addition, we ensure structural graph constraints using a simple set of heuristics. On the English blind test data, our system achieves a very high parsing accuracy, ranking 1st out of 10 with an ELAS F1 score of 88.94%.</p>
    </span>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge acl"><a href="https://aclanthology.org/venues/acl/" target="_blank" rel="external nofollow noopener">ACL</a></abbr>
    
  
  </div>

  <div id="grunewald-etal-2018-generalized" class="col-sm-8">
    
      <span class="title">Generalized Chart Constraints for Efficient PCFG and TAG Parsing</span>
      <span class="author">
        
          
            
              
                  Stefan Grünewald,
              
            
          
        
          
            
              
                  Sophie Henning,
              
            
          
        
          
            
              
                  and <a href="http://www.coli.uni-saarland.de/~koller/" target="_blank" rel="external nofollow noopener">Alexander Koller</a>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>
      
      <!--
        2018
      -->
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abstract</a>]
    
    
    
    
      [<a href="/assets/pdf/cc_paper.pdf" target="_blank">PDF</a>]
    
    
    
      [<a href="/assets/pdf/cc_poster.pdf" target="_blank">Poster</a>]
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Chart constraints, which specify at which string positions a constituent may begin or end, have been shown to speed up chart parsers for PCFGs. We generalize chart constraints to more expressive grammar formalisms and describe a neural tagger which predicts chart constraints at very high precision. Our constraints accelerate both PCFG and TAG parsing, and combine effectively with other pruning techniques (coarse-to-fine and supertagging) for an overall speedup of two orders of magnitude, while improving accuracy.</p>
    </span>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom">
  <div class="container">
    © Copyright 2025 Stefan Grünewald.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

    
  </div>
</footer>



  </body>

</html>
